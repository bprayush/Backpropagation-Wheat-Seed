{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1.0 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, architecture):\n",
    "        #architecture - numpy array with ith element representing the number of neurons in the ith layer.\n",
    "        \n",
    "        #Initialize the network architecture\n",
    "        self.L = architecture.size - 1 #L corresponds to the last layer of the network.\n",
    "        self.n = architecture #n stores the number of neurons in each layer\n",
    "        #input_size is the number of neurons in the first layer i.e. n[0]\n",
    "        #output_size is the number of neurons in the last layer i.e. n[L]\n",
    "        \n",
    "        #Parameters will store the network parameters, i.e. the weights and biases\n",
    "        self.parameters = {}\n",
    "        \n",
    "        #Initialize the network weights and biases:\n",
    "        for i in range (1, self.L + 1): \n",
    "            #Initialize weights to small random values\n",
    "            self.parameters['W' + str(i)] = np.random.randn(self.n[i], self.n[i - 1]) * 0.01\n",
    "            \n",
    "            #Initialize rest of the parameters to 1\n",
    "            self.parameters['b' + str(i)] = np.ones((self.n[i], 1))\n",
    "            self.parameters['z' + str(i)] = np.ones((self.n[i], 1))\n",
    "            self.parameters['a' + str(i)] = np.ones((self.n[i], 1))\n",
    "        \n",
    "        #As we started the loop from 1, we haven't initialized a[0]:\n",
    "        self.parameters['a0'] = np.ones((self.n[i], 1))\n",
    "        \n",
    "        #Initialize the cost:\n",
    "        self.parameters['C'] = 1\n",
    "        \n",
    "        #Create a dictionary for storing the derivatives:\n",
    "        self.derivatives = {}\n",
    "                    \n",
    "    def forward_propagate(self, X):\n",
    "        #Note that X here, is just one training example\n",
    "        self.parameters['a0'] = X\n",
    "        \n",
    "        #Calculate the activations for every layer l\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.parameters['z' + str(l)] = np.add(np.dot(self.parameters['W' + str(l)], self.parameters['a' + str(l - 1)]), self.parameters['b' + str(l)])\n",
    "            self.parameters['a' + str(l)] = sigmoid(self.parameters['z' + str(l)])\n",
    "        \n",
    "    def compute_cost(self, y):\n",
    "        self.parameters['C'] = -(y*np.log(self.parameters['a' + str(self.L)]) + (1-y)*np.log( 1 - self.parameters['a' + str(self.L)]))\n",
    "    \n",
    "    def compute_derivatives(self, y):\n",
    "        #Partial derivatives of the cost function with respect to z[L], W[L] and b[L]:        \n",
    "        #dzL\n",
    "        self.derivatives['dz' + str(self.L)] = self.parameters['a' + str(self.L)] - y\n",
    "        #dWL\n",
    "        self.derivatives['dW' + str(self.L)] = np.dot(self.derivatives['dz' + str(self.L)], np.transpose(self.parameters['a' + str(self.L - 1)]))\n",
    "        #dbL\n",
    "        self.derivatives['db' + str(self.L)] = self.derivatives['dz' + str(self.L)]\n",
    "\n",
    "        #Partial derivatives of the cost function with respect to z[l], W[l] and b[l]\n",
    "        for l in range(self.L-1, 0, -1):\n",
    "            self.derivatives['dz' + str(l)] = np.dot(np.transpose(self.parameters['W' + str(l + 1)]), self.derivatives['dz' + str(l + 1)])*sigmoid_prime(self.parameters['z' + str(l)])\n",
    "            self.derivatives['dW' + str(l)] = np.dot(self.derivatives['dz' + str(l)], np.transpose(self.parameters['a' + str(l - 1)]))\n",
    "            self.derivatives['db' + str(l)] = self.derivatives['dz' + str(l)]\n",
    "            \n",
    "    def update_parameters(self, alpha):\n",
    "        for l in range(1, self.L+1):\n",
    "            self.parameters['W' + str(l)] -= alpha*self.derivatives['dW' + str(l)]\n",
    "            self.parameters['b' + str(l)] -= alpha*self.derivatives['db' + str(l)]\n",
    "        \n",
    "    def predict(self, x):\n",
    "        self.forward_propagate(x)\n",
    "        return self.parameters['a' + str(self.L)]\n",
    "        \n",
    "    def fit(self, X, Y, num_iter, alpha = 0.01):\n",
    "        for iter in range(0, num_iter):\n",
    "            c = 0 #Stores the cost\n",
    "            n_c = 0 #Stores the number of correct predictions\n",
    "            \n",
    "            for i in range(0, X.shape[0]):\n",
    "              x = X[i].reshape((X[i].size, 1))\n",
    "              y = Y[i]\n",
    "\n",
    "              self.forward_propagate(x)\n",
    "              self.compute_cost(y)\n",
    "              self.compute_derivatives(y)\n",
    "              self.update_parameters(alpha)\n",
    "\n",
    "              c += self.parameters['C'] \n",
    "\n",
    "              y_pred = self.predict(x)\n",
    "              #y_pred is the probability, so to convert it into a class value:\n",
    "              y_pred = (y_pred > 0.5) \n",
    "\n",
    "              if y_pred == y:\n",
    "                  n_c += 1\n",
    "            \n",
    "            c = c/X.shape[0]\n",
    "            print('Iteration: ', iter)\n",
    "            print(\"Cost: \", c)\n",
    "            print(\"Accuracy:\", (n_c/X.shape[0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('wheat-seeds-binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_dataset['Class'] = shuffled_dataset['Class'] - 1\n",
    "\n",
    "X = shuffled_dataset.iloc[:, 0:-1].values\n",
    "y = shuffled_dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = StandardScaler()\n",
    "X = sc_X.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "architecture = np.array([7, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NeuralNetwork(architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Cost:  [[286.91532216]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  1\n",
      "Cost:  [[287.20195255]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  2\n",
      "Cost:  [[287.48858295]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  3\n",
      "Cost:  [[287.77521335]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  4\n",
      "Cost:  [[288.06184375]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  5\n",
      "Cost:  [[288.34847416]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  6\n",
      "Cost:  [[288.63510458]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  7\n",
      "Cost:  [[288.921735]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  8\n",
      "Cost:  [[289.20836542]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  9\n",
      "Cost:  [[289.49499585]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  10\n",
      "Cost:  [[289.78162628]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  11\n",
      "Cost:  [[290.06825672]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  12\n",
      "Cost:  [[290.35488716]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  13\n",
      "Cost:  [[290.64151761]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  14\n",
      "Cost:  [[290.92814806]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  15\n",
      "Cost:  [[291.21477851]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  16\n",
      "Cost:  [[291.50140897]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  17\n",
      "Cost:  [[291.78803943]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  18\n",
      "Cost:  [[292.0746699]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  19\n",
      "Cost:  [[292.36130038]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  20\n",
      "Cost:  [[292.64793085]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  21\n",
      "Cost:  [[292.93456133]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  22\n",
      "Cost:  [[293.22119182]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  23\n",
      "Cost:  [[293.50782231]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  24\n",
      "Cost:  [[293.7944528]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  25\n",
      "Cost:  [[294.0810833]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  26\n",
      "Cost:  [[294.3677138]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  27\n",
      "Cost:  [[294.65434431]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  28\n",
      "Cost:  [[294.94097482]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  29\n",
      "Cost:  [[295.22760534]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  30\n",
      "Cost:  [[295.51423586]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  31\n",
      "Cost:  [[295.80086638]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  32\n",
      "Cost:  [[296.08749691]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  33\n",
      "Cost:  [[296.37412744]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  34\n",
      "Cost:  [[296.66075798]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  35\n",
      "Cost:  [[296.94738852]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  36\n",
      "Cost:  [[297.23401907]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  37\n",
      "Cost:  [[297.52064962]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  38\n",
      "Cost:  [[297.80728017]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  39\n",
      "Cost:  [[298.09391073]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  40\n",
      "Cost:  [[298.38054129]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  41\n",
      "Cost:  [[298.66717186]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  42\n",
      "Cost:  [[298.95380243]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  43\n",
      "Cost:  [[299.240433]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  44\n",
      "Cost:  [[299.52706358]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  45\n",
      "Cost:  [[299.81369416]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  46\n",
      "Cost:  [[300.10032475]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  47\n",
      "Cost:  [[300.38695534]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  48\n",
      "Cost:  [[300.67358593]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  49\n",
      "Cost:  [[300.96021653]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  50\n",
      "Cost:  [[301.24684713]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  51\n",
      "Cost:  [[301.53347774]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  52\n",
      "Cost:  [[301.82010835]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  53\n",
      "Cost:  [[302.10673896]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  54\n",
      "Cost:  [[302.39336958]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  55\n",
      "Cost:  [[302.6800002]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  56\n",
      "Cost:  [[302.96663083]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  57\n",
      "Cost:  [[303.25326146]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  58\n",
      "Cost:  [[303.53989209]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  59\n",
      "Cost:  [[303.82652273]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  60\n",
      "Cost:  [[304.11315337]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  61\n",
      "Cost:  [[304.39978402]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  62\n",
      "Cost:  [[304.68641467]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  63\n",
      "Cost:  [[304.97304532]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  64\n",
      "Cost:  [[305.25967598]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  65\n",
      "Cost:  [[305.54630664]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  66\n",
      "Cost:  [[305.8329373]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  67\n",
      "Cost:  [[306.11956797]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  68\n",
      "Cost:  [[306.40619865]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  69\n",
      "Cost:  [[306.69282932]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  70\n",
      "Cost:  [[306.97946]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  71\n",
      "Cost:  [[307.26609069]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  72\n",
      "Cost:  [[307.55272137]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  73\n",
      "Cost:  [[307.83935207]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  74\n",
      "Cost:  [[308.12598276]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  75\n",
      "Cost:  [[308.41261346]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  76\n",
      "Cost:  [[308.69924416]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  77\n",
      "Cost:  [[308.98587487]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  78\n",
      "Cost:  [[309.27250558]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  79\n",
      "Cost:  [[309.55913629]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  80\n",
      "Cost:  [[309.84576701]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  81\n",
      "Cost:  [[310.13239773]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  82\n",
      "Cost:  [[310.41902846]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  83\n",
      "Cost:  [[310.70565918]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  84\n",
      "Cost:  [[310.99228992]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  85\n",
      "Cost:  [[311.27892065]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  86\n",
      "Cost:  [[311.56555139]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  87\n",
      "Cost:  [[311.85218213]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  88\n",
      "Cost:  [[312.13881288]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  89\n",
      "Cost:  [[312.42544363]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  90\n",
      "Cost:  [[312.71207438]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  91\n",
      "Cost:  [[312.99870514]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  92\n",
      "Cost:  [[313.2853359]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  93\n",
      "Cost:  [[313.57196666]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  94\n",
      "Cost:  [[313.85859743]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  95\n",
      "Cost:  [[314.1452282]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  96\n",
      "Cost:  [[314.43185898]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  97\n",
      "Cost:  [[314.71848976]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  98\n",
      "Cost:  [[315.00512054]]\n",
      "Accuracy: 45.91836734693878\n",
      "Iteration:  99\n",
      "Cost:  [[315.29175132]]\n",
      "Accuracy: 45.91836734693878\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, 100, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_c = 0\n",
    "for i in range(0, X_test.shape[0]):\n",
    "    x = X_test[i].reshape((X_test[i].size, 1))\n",
    "    y = y_test[i]\n",
    "    y_pred = classifier.predict(x)\n",
    "    y_pred = (y_pred > 0.5)\n",
    "    if y_pred == y:\n",
    "        n_c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 59.523809523809526\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy\", (n_c/X_test.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
